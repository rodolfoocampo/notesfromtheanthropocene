The first wave of [[artificial intelligence]] (AI) was based on symbolic systems. Rules operated on this symbols that represented concepts in the world coded by the system's designers. These systems did not learn, but were still powerful enough to beat humans at certain tasks. Deep Blue beating Kasparov is one such example of rule-based symbolic systems. 

The next wave of AI was based on learning. In particular machines learning from data by incorporating feedback. Based on large training datasets, algorithms were set up with the goal of performing a particular task. The algorithms then receive feedback, in the form of reward or parameter adjustments reflecting if the task was performed correctly. This wave elevated machines to new capabilities that were beyond rule-based sytems. For example, classification images or voice. These tasks are too complex and it would be impossible to code the rule for every possible scenario. However, machine still lacked capabiltiies associaated with humans, like creativity or producing coherent language. 

A third wave that seems to be underway with models that incorporate learning via feedback but which not only learn input-output, but also learn representations or symbols along the way. These systems include auto-encodders, generative adversarial networks and transformers. These models create compress representations of its input. For example, 500x500 pixel images get encoded in 128-valued vector. This 128 contains the necessary information to understand the basic concept present in a 500x500 image. With this, the computer learns concepts of the world, and operated at that level. 

Arguably, the same happens with out brain. Perhaps the first, most basic forms of life were simple statistic machines, driven by punishment and reward, learning the rules of what behaviors produce which rewards. But eventually, our brains found it useful to create representations of the world and operate at the level of symbols: present, past, god, future, family, death, eventually evolving language as a shared code to talk about this internal representations that we all share. These concepts encapsulate a lot of information about the world and allow us to perform more complex tasks, and more importantly, engage in complex social behavior. 

These symbols live in [[latent spaces]] where high-dimensional perceptions are projected into a lower dimensional, high information density value. For example, when we see a rabbit, it is a high dimensional image containing lots of information about its size, color, shape of every hair, ears, etcetera. These inputs go into our head through our eyes, and our brain turns that into a lower dimensional representation of rabbit. We don't have to evaluate every aspect of what we are seeing to know we see a rabbit. This is because a latent space, by nature, is hidden. Its values existing at a lower dimension that perception. 

#artificialintelligenceaugmentation 
#artificialintelligence 
#brain
#cybernetics

Related:

[[Difference between Cybernetics and AI]]